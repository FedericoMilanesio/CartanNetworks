{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def format_mean_sem(mean, sem):\n",
    "    if sem == 0 or np.isnan(sem):\n",
    "        return f\"{mean:.4f}(0)\"  # fallback\n",
    "    \n",
    "    exponent = int(math.floor(np.log10(sem)))\n",
    "    precision = -exponent\n",
    "    digits = max(0, precision)\n",
    "\n",
    "    sem_rounded = round(sem, digits)\n",
    "    mean_rounded = round(mean, digits)\n",
    "    sem_for_latex = int(round(sem_rounded * (10 ** digits)))\n",
    "\n",
    "    return f\"{mean_rounded:.{digits}f}({sem_for_latex})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_dataset(p: Path):\n",
    "    dfs = []\n",
    "    for folder in p.glob('*'):\n",
    "        for file in folder.glob('*.csv'):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "            dfs[-1]['Seed'] = int(re.findall(r'\\d+', file.name)[0])\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df['Loglr'] = df['Learning rate'].apply(lambda x: np.log10(x))\n",
    "\n",
    "    activation_dict = {\n",
    "        'activation.identity': \"Identity\",\n",
    "        'activation.relu': \"ReLU\",\n",
    "        'activation.leaky_relu': \"LeakyReLU\",\n",
    "        'activation.sigmoid': \"Sigmoid\",\n",
    "        'activation.dmelu': \"DELU\"\n",
    "    }\n",
    "\n",
    "    model_dict = {\n",
    "        \"model_type.hyperbolic\" : \"Hyperbolic\",\n",
    "        \"model_type.euclidean\" : \"Euclidean\",\n",
    "        \"model_type.eubn\" : \"Eucl + BN\",\n",
    "        \"model_type.poincare\" : \"Poincare\",\n",
    "        \"model_type.lorentz\" : \"Lorentz\",\n",
    "        \"model_type.logr\" : \"Logistic\"\n",
    "    }\n",
    "\n",
    "    df['Model'] = df['Model'].apply(lambda x: model_dict[x])\n",
    "    df['Activation'] = df['Activation'].apply(lambda x: activation_dict[x])\n",
    "    df['Type'] = df['Model'] + ' - ' + df['Activation']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_462662/1550530924.py:6: FutureWarning: The provided callable <built-in function min> is currently using np.minimum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.minimum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: FutureWarning: The provided callable <built-in function min> is currently using np.minimum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.minimum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: FutureWarning: The provided callable <built-in function min> is currently using np.minimum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.minimum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: FutureWarning: The provided callable <built-in function min> is currently using np.minimum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.minimum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: FutureWarning: The provided callable <built-in function min> is currently using np.minimum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.minimum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
      "/tmp/ipykernel_462662/1550530924.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for problem in ['sinc', 'sinc3', 'prod2', 'prod3', 'hyp']:\n",
    "    df = build_dataset(Path('..//data/regression')/problem)\n",
    "    df['Type'] = df['Model'] + '_' + df['Activation']\n",
    "    df['Type'] = df['Type'].apply(lambda x: ' '.join(x.split('_')))\n",
    "    df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(min)\n",
    "    df1['Problem'] = problem\n",
    "    dfs.append(df1)\n",
    "\n",
    "regression = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression['R^2'] = 1 - regression['Normalized test loss']\n",
    "\n",
    "loss_summary = (\n",
    "    regression\n",
    "    .groupby(['Problem', 'Type'])['R^2']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Mean_R^2', 'std': 'Std_R^2', 'count': 'N'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Type</th>\n",
       "      <th>Mean_R^2</th>\n",
       "      <th>Std_R^2</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.853256</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.972528</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.931343</td>\n",
       "      <td>0.075905</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.965276</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.999485</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.991176</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.990699</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.859017</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.979331</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.988278</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.656016</td>\n",
       "      <td>0.251743</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.783343</td>\n",
       "      <td>0.056880</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.910531</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.932533</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.644475</td>\n",
       "      <td>0.272034</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Problem                 Type  Mean_R^2   Std_R^2   N\n",
       "0      hyp       Euclidean DELU  0.853256  0.017045  10\n",
       "1      hyp      Hyperbolic DELU  0.972528  0.004905  10\n",
       "2      hyp  Hyperbolic Identity  0.931343  0.075905  10\n",
       "3      hyp        Poincare DELU  0.965276  0.005898  10\n",
       "4    prod2       Euclidean DELU  0.999126  0.000255  10\n",
       "5    prod2      Hyperbolic DELU  0.999485  0.000093  10\n",
       "6    prod2  Hyperbolic Identity  0.999584  0.000127  10\n",
       "7    prod2        Poincare DELU  0.997436  0.000497  10\n",
       "8    prod3       Euclidean DELU  0.991176  0.001982  10\n",
       "9    prod3      Hyperbolic DELU  0.990699  0.003797  10\n",
       "10   prod3  Hyperbolic Identity  0.984788  0.002848  10\n",
       "11   prod3        Poincare DELU  0.988764  0.001967  10\n",
       "12    sinc       Euclidean DELU  0.859017  0.020588  10\n",
       "13    sinc      Hyperbolic DELU  0.979331  0.004263  10\n",
       "14    sinc  Hyperbolic Identity  0.988278  0.001636  10\n",
       "15    sinc        Poincare DELU  0.656016  0.251743  10\n",
       "16   sinc3       Euclidean DELU  0.783343  0.056880  10\n",
       "17   sinc3      Hyperbolic DELU  0.910531  0.012329  10\n",
       "18   sinc3  Hyperbolic Identity  0.932533  0.008399  10\n",
       "19   sinc3        Poincare DELU  0.644475  0.272034  10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_results = []\n",
    "\n",
    "for problem, group in regression.groupby(\"Problem\"):\n",
    "\n",
    "    r2_means = group.groupby(\"Type\")[\"R^2\"].mean()\n",
    "    best_type = r2_means.idxmax()\n",
    "    \n",
    "    best_scores = group[group[\"Type\"] == best_type][\"R^2\"]\n",
    "    \n",
    "    for type_ in group[\"Type\"].unique():\n",
    "        if type_ == best_type:\n",
    "            continue\n",
    "        \n",
    "        comp_scores = group[group[\"Type\"] == type_][\"R^2\"]\n",
    "        \n",
    "        t_stat, p_val = ttest_ind(best_scores, comp_scores, equal_var=False)  # Welch's t-test\n",
    "        \n",
    "        ttest_results.append({\n",
    "            \"Problem\": problem,\n",
    "            \"Best_Type\": best_type,\n",
    "            \"Compared_Type\": type_,\n",
    "            \"T_stat\": t_stat,\n",
    "            \"P_value\": p_val,\n",
    "            \"Best_Mean\": best_scores.mean(),\n",
    "            \"Compared_Mean\": comp_scores.mean(),\n",
    "            \"Best_Better\": best_scores.mean() > comp_scores.mean() and p_val < 0.05\n",
    "        })\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Best_Type</th>\n",
       "      <th>Compared_Type</th>\n",
       "      <th>T_stat</th>\n",
       "      <th>P_value</th>\n",
       "      <th>Best_Mean</th>\n",
       "      <th>Compared_Mean</th>\n",
       "      <th>Best_Better</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>21.264652</td>\n",
       "      <td>5.833632e-10</td>\n",
       "      <td>0.972528</td>\n",
       "      <td>0.853256</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>1.712240</td>\n",
       "      <td>1.207287e-01</td>\n",
       "      <td>0.972528</td>\n",
       "      <td>0.931343</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyp</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>2.989565</td>\n",
       "      <td>8.071866e-03</td>\n",
       "      <td>0.972528</td>\n",
       "      <td>0.965276</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>5.079860</td>\n",
       "      <td>2.006205e-04</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>1.981341</td>\n",
       "      <td>6.444789e-02</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.999485</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prod2</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>13.238020</td>\n",
       "      <td>9.675146e-08</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.352386</td>\n",
       "      <td>7.299634e-01</td>\n",
       "      <td>0.991176</td>\n",
       "      <td>0.990699</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>5.821767</td>\n",
       "      <td>2.560083e-05</td>\n",
       "      <td>0.991176</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prod3</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>2.731676</td>\n",
       "      <td>1.369760e-02</td>\n",
       "      <td>0.991176</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>19.792173</td>\n",
       "      <td>8.435626e-09</td>\n",
       "      <td>0.988278</td>\n",
       "      <td>0.859017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>6.195528</td>\n",
       "      <td>5.369603e-05</td>\n",
       "      <td>0.988278</td>\n",
       "      <td>0.979331</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sinc</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>4.173629</td>\n",
       "      <td>2.398216e-03</td>\n",
       "      <td>0.988278</td>\n",
       "      <td>0.656016</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>8.205283</td>\n",
       "      <td>1.392156e-05</td>\n",
       "      <td>0.932533</td>\n",
       "      <td>0.783343</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>4.663964</td>\n",
       "      <td>2.646140e-04</td>\n",
       "      <td>0.932533</td>\n",
       "      <td>0.910531</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sinc3</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>3.346956</td>\n",
       "      <td>8.542057e-03</td>\n",
       "      <td>0.932533</td>\n",
       "      <td>0.644475</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Problem            Best_Type        Compared_Type     T_stat       P_value  \\\n",
       "0      hyp      Hyperbolic DELU       Euclidean DELU  21.264652  5.833632e-10   \n",
       "1      hyp      Hyperbolic DELU  Hyperbolic Identity   1.712240  1.207287e-01   \n",
       "2      hyp      Hyperbolic DELU        Poincare DELU   2.989565  8.071866e-03   \n",
       "3    prod2  Hyperbolic Identity       Euclidean DELU   5.079860  2.006205e-04   \n",
       "4    prod2  Hyperbolic Identity      Hyperbolic DELU   1.981341  6.444789e-02   \n",
       "5    prod2  Hyperbolic Identity        Poincare DELU  13.238020  9.675146e-08   \n",
       "6    prod3       Euclidean DELU      Hyperbolic DELU   0.352386  7.299634e-01   \n",
       "7    prod3       Euclidean DELU  Hyperbolic Identity   5.821767  2.560083e-05   \n",
       "8    prod3       Euclidean DELU        Poincare DELU   2.731676  1.369760e-02   \n",
       "9     sinc  Hyperbolic Identity       Euclidean DELU  19.792173  8.435626e-09   \n",
       "10    sinc  Hyperbolic Identity      Hyperbolic DELU   6.195528  5.369603e-05   \n",
       "11    sinc  Hyperbolic Identity        Poincare DELU   4.173629  2.398216e-03   \n",
       "12   sinc3  Hyperbolic Identity       Euclidean DELU   8.205283  1.392156e-05   \n",
       "13   sinc3  Hyperbolic Identity      Hyperbolic DELU   4.663964  2.646140e-04   \n",
       "14   sinc3  Hyperbolic Identity        Poincare DELU   3.346956  8.542057e-03   \n",
       "\n",
       "    Best_Mean  Compared_Mean  Best_Better  \n",
       "0    0.972528       0.853256         True  \n",
       "1    0.972528       0.931343        False  \n",
       "2    0.972528       0.965276         True  \n",
       "3    0.999584       0.999126         True  \n",
       "4    0.999584       0.999485        False  \n",
       "5    0.999584       0.997436         True  \n",
       "6    0.991176       0.990699        False  \n",
       "7    0.991176       0.984788         True  \n",
       "8    0.991176       0.988764         True  \n",
       "9    0.988278       0.859017         True  \n",
       "10   0.988278       0.979331         True  \n",
       "11   0.988278       0.656016         True  \n",
       "12   0.932533       0.783343         True  \n",
       "13   0.932533       0.910531         True  \n",
       "14   0.932533       0.644475         True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/tmp/ipykernel_462662/1594675400.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  problems_dict = {\"sinc\": \"$\\mathrm{Sinc}(\\|x\\|_2)$\",\n",
      "/tmp/ipykernel_462662/1594675400.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"sinc3\":\"$\\mathrm{Sinc}(\\|x\\|_3)$\",\n",
      "/tmp/ipykernel_462662/1594675400.py:5: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  \"hyp\":\"$\\\\frac{1}{n}\\left(\\sum_i^{n-1} x_i^2 - x_n^2\\\\right)$\"\n"
     ]
    }
   ],
   "source": [
    "problems_dict = {\"sinc\": \"$\\mathrm{Sinc}(\\|x\\|_2)$\", \n",
    "                 \"sinc3\":\"$\\mathrm{Sinc}(\\|x\\|_3)$\", \n",
    "                 \"prod3\":\"$x_0 + x_0x_1 + x_0x_1x_2$\", \n",
    "                 \"prod2\":\"$x_0 + x_0x_1$\", \n",
    "                 \"hyp\":\"$\\\\frac{1}{n}\\left(\\sum_i^{n-1} x_i^2 - x_n^2\\\\right)$\"\n",
    "                 }\n",
    "\n",
    "models_dict = {\"Poincare DELU\": \"Poincaré + DiLU\",\n",
    "               \"Hyperbolic Identity\": \"Cartan\",\n",
    "               \"Hyperbolic DELU\": \"Cartan + DiLU\",\n",
    "               \"Euclidean DELU\": \"Euclidean + DiLU\",\n",
    "               \"Lorentz DELU\": \"Lorentz + DiLU\",\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = loss_summary[\"N\"].mean()\n",
    "loss_summary['SEM'] = loss_summary['Std_R^2'] / np.sqrt(n)\n",
    "\n",
    "problems = loss_summary['Problem'].unique()\n",
    "types = loss_summary['Type'].unique()\n",
    "\n",
    "best_types_per_problem = {}\n",
    "for prob in problems:\n",
    "    sig_bests = ttest_df[(ttest_df[\"Problem\"] == prob) & (ttest_df[\"Best_Better\"] == True)]\n",
    "    if not sig_bests.empty:\n",
    "        best_type = sig_bests.iloc[0][\"Best_Type\"]\n",
    "    else:\n",
    "        best_type = loss_summary[loss_summary[\"Problem\"] == prob].sort_values(\"R^2\", ascending=False).iloc[0][\"Type\"]\n",
    "    best_types_per_problem[prob] = best_type\n",
    "\n",
    "header = r\"\"\"\\centering\n",
    "\\caption{$R^2$ on toy regression datasets (mean $\\pm$ std, $n$ = \"\"\" + str(int(n)) + r\"\"\")}\\label{tab:regression}\n",
    "\\begin{tabular}{\n",
    "  l\"\"\" + \"  \" + \"  \".join([\"S[table-format=1.3(2)]\" for _ in types]) + r\"\"\"}\n",
    "\\toprule\n",
    "\\textbf{Problem} & \"\"\" + \" & \".join([f\"{{{models_dict[t]}}}\" for t in types]) + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "rows = []\n",
    "for prob in problems:\n",
    "    row = [problems_dict[prob]]\n",
    "    for t in types:\n",
    "        entry = loss_summary[(loss_summary['Problem'] == prob) & (loss_summary['Type'] == t)]\n",
    "        if not entry.empty:\n",
    "            mean = entry['Mean_R^2'].values[0]\n",
    "            sem = entry['Std_R^2'].values[0]\n",
    "            formatted = format_mean_sem(mean, sem)\n",
    "            if best_types_per_problem[prob] == t:\n",
    "                formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "            else:\n",
    "                formatted = r\"\\num{\" + formatted + \"}\"\n",
    "        else:\n",
    "            formatted = \"-\"\n",
    "        row.append(formatted)\n",
    "    rows.append(\" & \".join(row) + r\" \\\\\")\n",
    "\n",
    "footer = r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\"\"\"\n",
    "\n",
    "latex_table = header + \"\\n\".join(rows) + \"\\n\" + footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"regression.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_462662/279080215.py:7: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/279080215.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Model</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Train loss</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Weight decay</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Nlayers</th>\n",
       "      <th>Train hyperbolicities</th>\n",
       "      <th>Test hyperbolicities</th>\n",
       "      <th>Time</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Loglr</th>\n",
       "      <th>Type</th>\n",
       "      <th>Problem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146</td>\n",
       "      <td>Euclidean</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.9646</td>\n",
       "      <td>0.801424</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>1.457546</td>\n",
       "      <td>146</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1379.658705</td>\n",
       "      <td>66</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>mnist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171</td>\n",
       "      <td>Euclidean</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.9651</td>\n",
       "      <td>0.829703</td>\n",
       "      <td>0.981357</td>\n",
       "      <td>1.501590</td>\n",
       "      <td>171</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600.686154</td>\n",
       "      <td>454</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>mnist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139</td>\n",
       "      <td>Euclidean</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.9637</td>\n",
       "      <td>0.794452</td>\n",
       "      <td>0.980023</td>\n",
       "      <td>1.476443</td>\n",
       "      <td>139</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1312.862182</td>\n",
       "      <td>1510</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>mnist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168</td>\n",
       "      <td>Euclidean</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>0.816425</td>\n",
       "      <td>0.980890</td>\n",
       "      <td>1.504859</td>\n",
       "      <td>168</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1553.072401</td>\n",
       "      <td>3682</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>mnist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>Euclidean</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>0.791865</td>\n",
       "      <td>0.977872</td>\n",
       "      <td>1.507620</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1477.297243</td>\n",
       "      <td>4932</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>mnist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>217</td>\n",
       "      <td>Poincare</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.4752</td>\n",
       "      <td>2.029708</td>\n",
       "      <td>0.509363</td>\n",
       "      <td>2.155364</td>\n",
       "      <td>217</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4226.195787</td>\n",
       "      <td>4524</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>200</td>\n",
       "      <td>Poincare</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>2.039002</td>\n",
       "      <td>0.515745</td>\n",
       "      <td>2.161585</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3937.594336</td>\n",
       "      <td>5036</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>224</td>\n",
       "      <td>Poincare</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.4788</td>\n",
       "      <td>2.031615</td>\n",
       "      <td>0.518886</td>\n",
       "      <td>2.158645</td>\n",
       "      <td>224</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5178.055401</td>\n",
       "      <td>6004</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>196</td>\n",
       "      <td>Poincare</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.4771</td>\n",
       "      <td>2.066206</td>\n",
       "      <td>0.512184</td>\n",
       "      <td>2.169173</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3889.882892</td>\n",
       "      <td>6416</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>223</td>\n",
       "      <td>Poincare</td>\n",
       "      <td>DELU</td>\n",
       "      <td>0.4733</td>\n",
       "      <td>2.046469</td>\n",
       "      <td>0.513364</td>\n",
       "      <td>2.142704</td>\n",
       "      <td>223</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4424.283114</td>\n",
       "      <td>8497</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0      Model Activation  Test accuracy  Test loss  \\\n",
       "0          146  Euclidean       DELU         0.9646   0.801424   \n",
       "1          171  Euclidean       DELU         0.9651   0.829703   \n",
       "2          139  Euclidean       DELU         0.9637   0.794452   \n",
       "3          168  Euclidean       DELU         0.9657   0.816425   \n",
       "4          160  Euclidean       DELU         0.9620   0.791865   \n",
       "..         ...        ...        ...            ...        ...   \n",
       "45         217   Poincare       DELU         0.4752   2.029708   \n",
       "46         200   Poincare       DELU         0.4766   2.039002   \n",
       "47         224   Poincare       DELU         0.4788   2.031615   \n",
       "48         196   Poincare       DELU         0.4771   2.066206   \n",
       "49         223   Poincare       DELU         0.4733   2.046469   \n",
       "\n",
       "    Train accuracy  Train loss  Epoch  Learning rate  Weight decay  Neurons  \\\n",
       "0         0.977388    1.457546    146         0.0001       0.00001       20   \n",
       "1         0.981357    1.501590    171         0.0001       0.00001       20   \n",
       "2         0.980023    1.476443    139         0.0001       0.00001       20   \n",
       "3         0.980890    1.504859    168         0.0001       0.00001       20   \n",
       "4         0.977872    1.507620    160         0.0001       0.00001       20   \n",
       "..             ...         ...    ...            ...           ...      ...   \n",
       "45        0.509363    2.155364    217         0.0001       0.00001       20   \n",
       "46        0.515745    2.161585    200         0.0001       0.00001       20   \n",
       "47        0.518886    2.158645    224         0.0001       0.00001       20   \n",
       "48        0.512184    2.169173    196         0.0001       0.00001       20   \n",
       "49        0.513364    2.142704    223         0.0001       0.00001       20   \n",
       "\n",
       "    Nlayers  Train hyperbolicities  Test hyperbolicities         Time  Seed  \\\n",
       "0         2                    NaN                   NaN  1379.658705    66   \n",
       "1         2                    NaN                   NaN  1600.686154   454   \n",
       "2         2                    NaN                   NaN  1312.862182  1510   \n",
       "3         2                    NaN                   NaN  1553.072401  3682   \n",
       "4         2                    NaN                   NaN  1477.297243  4932   \n",
       "..      ...                    ...                   ...          ...   ...   \n",
       "45        2                    NaN                   NaN  4226.195787  4524   \n",
       "46        2                    NaN                   NaN  3937.594336  5036   \n",
       "47        2                    NaN                   NaN  5178.055401  6004   \n",
       "48        2                    NaN                   NaN  3889.882892  6416   \n",
       "49        2                    NaN                   NaN  4424.283114  8497   \n",
       "\n",
       "    Loglr            Type  Problem  \n",
       "0    -4.0  Euclidean DELU    mnist  \n",
       "1    -4.0  Euclidean DELU    mnist  \n",
       "2    -4.0  Euclidean DELU    mnist  \n",
       "3    -4.0  Euclidean DELU    mnist  \n",
       "4    -4.0  Euclidean DELU    mnist  \n",
       "..    ...             ...      ...  \n",
       "45   -4.0   Poincare DELU  cifar10  \n",
       "46   -4.0   Poincare DELU  cifar10  \n",
       "47   -4.0   Poincare DELU  cifar10  \n",
       "48   -4.0   Poincare DELU  cifar10  \n",
       "49   -4.0   Poincare DELU  cifar10  \n",
       "\n",
       "[200 rows x 19 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for problem in ['mnist', 'fmnist', 'kmnist', 'cifar10']:\n",
    "    df = build_dataset(Path(\"../data/classification\")/problem)\n",
    "    df['Type'] = df['Model'] + '_' + df['Activation']\n",
    "    df['Type'] = df['Type'].apply(lambda x: ' '.join(x.split('_')))\n",
    "    df['Problem'] = problem\n",
    "    df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Seed', 'Type', 'Neurons'], as_index=False).apply(max)\n",
    "    dfs.append(df1)\n",
    "\n",
    "classification = pd.concat(dfs)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary = (\n",
    "    classification\n",
    "    .groupby(['Problem', 'Type'])['Test accuracy']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Mean_accuracy', 'std': 'Std_accuracy', 'count': 'N'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Type</th>\n",
       "      <th>Mean_accuracy</th>\n",
       "      <th>Std_accuracy</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.47728</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.47557</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.44391</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>0.41036</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.47478</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.86823</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.86906</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.85588</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>0.84899</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.87363</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.81401</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.80750</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.76968</td>\n",
       "      <td>0.013110</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>0.70383</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.82109</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>0.96471</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>0.96162</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>0.95386</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>0.92843</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>0.96366</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Problem                 Type  Mean_accuracy  Std_accuracy   N\n",
       "0   cifar10       Euclidean DELU        0.47728      0.003342  10\n",
       "1   cifar10      Hyperbolic DELU        0.47557      0.003210  10\n",
       "2   cifar10  Hyperbolic Identity        0.44391      0.007259  10\n",
       "3   cifar10    Logistic Identity        0.41036      0.001729  10\n",
       "4   cifar10        Poincare DELU        0.47478      0.002962  10\n",
       "5    fmnist       Euclidean DELU        0.86823      0.002337  10\n",
       "6    fmnist      Hyperbolic DELU        0.86906      0.001828  10\n",
       "7    fmnist  Hyperbolic Identity        0.85588      0.002997  10\n",
       "8    fmnist    Logistic Identity        0.84899      0.000528  10\n",
       "9    fmnist        Poincare DELU        0.87363      0.001989  10\n",
       "10   kmnist       Euclidean DELU        0.81401      0.005570  10\n",
       "11   kmnist      Hyperbolic DELU        0.80750      0.005952  10\n",
       "12   kmnist  Hyperbolic Identity        0.76968      0.013110  10\n",
       "13   kmnist    Logistic Identity        0.70383      0.000643  10\n",
       "14   kmnist        Poincare DELU        0.82109      0.005178  10\n",
       "15    mnist       Euclidean DELU        0.96471      0.001425  10\n",
       "16    mnist      Hyperbolic DELU        0.96162      0.002561  10\n",
       "17    mnist  Hyperbolic Identity        0.95386      0.002783  10\n",
       "18    mnist    Logistic Identity        0.92843      0.000279  10\n",
       "19    mnist        Poincare DELU        0.96366      0.001615  10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_results = []\n",
    "\n",
    "for problem, group in classification.groupby(\"Problem\"):\n",
    "\n",
    "    classification_means = group.groupby(\"Type\")[\"Test accuracy\"].mean()\n",
    "    best_type = classification_means.idxmax()\n",
    "    \n",
    "    best_scores = group[group[\"Type\"] == best_type][\"Test accuracy\"]\n",
    "    \n",
    "    for type_ in group[\"Type\"].unique():\n",
    "        if type_ == best_type:\n",
    "            continue\n",
    "        \n",
    "        comp_scores = group[group[\"Type\"] == type_][\"Test accuracy\"]\n",
    "        \n",
    "        t_stat, p_val = ttest_ind(best_scores, comp_scores, equal_var=False)  # Welch's t-test\n",
    "        \n",
    "        ttest_results.append({\n",
    "            \"Problem\": problem,\n",
    "            \"Best_Type\": best_type,\n",
    "            \"Compared_Type\": type_,\n",
    "            \"T_stat\": t_stat,\n",
    "            \"P_value\": p_val,\n",
    "            \"Best_Mean\": best_scores.mean(),\n",
    "            \"Compared_Mean\": comp_scores.mean(),\n",
    "            \"Best_Better\": best_scores.mean() > comp_scores.mean() and p_val < 0.05\n",
    "        })\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Best_Type</th>\n",
       "      <th>Compared_Type</th>\n",
       "      <th>T_stat</th>\n",
       "      <th>P_value</th>\n",
       "      <th>Best_Mean</th>\n",
       "      <th>Compared_Mean</th>\n",
       "      <th>Best_Better</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>1.167007</td>\n",
       "      <td>2.584601e-01</td>\n",
       "      <td>0.47728</td>\n",
       "      <td>0.47557</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>13.205037</td>\n",
       "      <td>9.041352e-09</td>\n",
       "      <td>0.47728</td>\n",
       "      <td>0.44391</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>56.246045</td>\n",
       "      <td>2.108429e-17</td>\n",
       "      <td>0.47728</td>\n",
       "      <td>0.41036</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cifar10</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>1.770561</td>\n",
       "      <td>9.381170e-02</td>\n",
       "      <td>0.47728</td>\n",
       "      <td>0.47478</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>5.563699</td>\n",
       "      <td>3.053853e-05</td>\n",
       "      <td>0.87363</td>\n",
       "      <td>0.86823</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>5.349385</td>\n",
       "      <td>4.491689e-05</td>\n",
       "      <td>0.87363</td>\n",
       "      <td>0.86906</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>15.604018</td>\n",
       "      <td>5.946313e-11</td>\n",
       "      <td>0.87363</td>\n",
       "      <td>0.85588</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>37.855506</td>\n",
       "      <td>2.319778e-12</td>\n",
       "      <td>0.87363</td>\n",
       "      <td>0.84899</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>2.943935</td>\n",
       "      <td>8.715280e-03</td>\n",
       "      <td>0.82109</td>\n",
       "      <td>0.81401</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>5.447213</td>\n",
       "      <td>3.806301e-05</td>\n",
       "      <td>0.82109</td>\n",
       "      <td>0.80750</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>11.533747</td>\n",
       "      <td>9.305026e-08</td>\n",
       "      <td>0.82109</td>\n",
       "      <td>0.76968</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kmnist</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>71.063106</td>\n",
       "      <td>5.155248e-14</td>\n",
       "      <td>0.82109</td>\n",
       "      <td>0.70383</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Hyperbolic DELU</td>\n",
       "      <td>3.334086</td>\n",
       "      <td>4.882232e-03</td>\n",
       "      <td>0.96471</td>\n",
       "      <td>0.96162</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Hyperbolic Identity</td>\n",
       "      <td>10.973050</td>\n",
       "      <td>4.467036e-08</td>\n",
       "      <td>0.96471</td>\n",
       "      <td>0.95386</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Logistic Identity</td>\n",
       "      <td>79.022580</td>\n",
       "      <td>6.088758e-15</td>\n",
       "      <td>0.96471</td>\n",
       "      <td>0.92843</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mnist</td>\n",
       "      <td>Euclidean DELU</td>\n",
       "      <td>Poincare DELU</td>\n",
       "      <td>1.541584</td>\n",
       "      <td>1.408399e-01</td>\n",
       "      <td>0.96471</td>\n",
       "      <td>0.96366</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Problem       Best_Type        Compared_Type     T_stat       P_value  \\\n",
       "0   cifar10  Euclidean DELU      Hyperbolic DELU   1.167007  2.584601e-01   \n",
       "1   cifar10  Euclidean DELU  Hyperbolic Identity  13.205037  9.041352e-09   \n",
       "2   cifar10  Euclidean DELU    Logistic Identity  56.246045  2.108429e-17   \n",
       "3   cifar10  Euclidean DELU        Poincare DELU   1.770561  9.381170e-02   \n",
       "4    fmnist   Poincare DELU       Euclidean DELU   5.563699  3.053853e-05   \n",
       "5    fmnist   Poincare DELU      Hyperbolic DELU   5.349385  4.491689e-05   \n",
       "6    fmnist   Poincare DELU  Hyperbolic Identity  15.604018  5.946313e-11   \n",
       "7    fmnist   Poincare DELU    Logistic Identity  37.855506  2.319778e-12   \n",
       "8    kmnist   Poincare DELU       Euclidean DELU   2.943935  8.715280e-03   \n",
       "9    kmnist   Poincare DELU      Hyperbolic DELU   5.447213  3.806301e-05   \n",
       "10   kmnist   Poincare DELU  Hyperbolic Identity  11.533747  9.305026e-08   \n",
       "11   kmnist   Poincare DELU    Logistic Identity  71.063106  5.155248e-14   \n",
       "12    mnist  Euclidean DELU      Hyperbolic DELU   3.334086  4.882232e-03   \n",
       "13    mnist  Euclidean DELU  Hyperbolic Identity  10.973050  4.467036e-08   \n",
       "14    mnist  Euclidean DELU    Logistic Identity  79.022580  6.088758e-15   \n",
       "15    mnist  Euclidean DELU        Poincare DELU   1.541584  1.408399e-01   \n",
       "\n",
       "    Best_Mean  Compared_Mean  Best_Better  \n",
       "0     0.47728        0.47557        False  \n",
       "1     0.47728        0.44391         True  \n",
       "2     0.47728        0.41036         True  \n",
       "3     0.47728        0.47478        False  \n",
       "4     0.87363        0.86823         True  \n",
       "5     0.87363        0.86906         True  \n",
       "6     0.87363        0.85588         True  \n",
       "7     0.87363        0.84899         True  \n",
       "8     0.82109        0.81401         True  \n",
       "9     0.82109        0.80750         True  \n",
       "10    0.82109        0.76968         True  \n",
       "11    0.82109        0.70383         True  \n",
       "12    0.96471        0.96162         True  \n",
       "13    0.96471        0.95386         True  \n",
       "14    0.96471        0.92843         True  \n",
       "15    0.96471        0.96366        False  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_dict = {\"cifar10\": \"Cifar10\", \n",
    "                 \"fmnist\":\"FMNIST\", \n",
    "                 \"kmnist\":\"KMNIST\", \n",
    "                 \"mnist\":\"MNIST\"\n",
    "                 }\n",
    "\n",
    "models_dict = {\"Poincare DELU\": \"Poincaré + DiLU\",\n",
    "               \"Hyperbolic Identity\": \"Cartan\",\n",
    "               \"Hyperbolic DELU\": \"Cartan + DiLU\",\n",
    "               \"Euclidean DELU\": \"Euclidean + DiLU\",\n",
    "               \"Lorentz DELU\": \"Lorentz + DiLU\",\n",
    "               \"Logistic Identity\": \"Logistic\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cifar10': 'Euclidean DELU',\n",
       " 'fmnist': 'Poincare DELU',\n",
       " 'kmnist': 'Poincare DELU',\n",
       " 'mnist': 'Euclidean DELU'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_types_per_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = loss_summary[\"N\"].mean()\n",
    "\n",
    "problems = loss_summary['Problem'].unique()\n",
    "types = loss_summary['Type'].unique()\n",
    "\n",
    "best_types_per_problem = {}\n",
    "for prob in problems:\n",
    "    sig_bests = ttest_df[(ttest_df[\"Problem\"] == prob) & (ttest_df[\"Best_Better\"] == True)]\n",
    "    if not sig_bests.empty:\n",
    "        best_type = sig_bests.iloc[0][\"Best_Type\"]\n",
    "    else:\n",
    "        best_type = loss_summary[loss_summary[\"Problem\"] == prob].sort_values(\"R^2\", ascending=False).iloc[0][\"Type\"]\n",
    "    best_types_per_problem[prob] = best_type\n",
    "\n",
    "header = r\"\"\"\\centering\n",
    "\\caption{Accuracy on real-world datasets (mean $\\pm$ std, $n$ = \"\"\" + str(int(n)) + r\"\"\")}\\label{tab:classification}\n",
    "\\begin{tabular}{\n",
    "  l\"\"\" + \"  \" + \"  \".join([\"S[table-format=1.3(2)]\" for _ in types]) + r\"\"\"}\n",
    "\\toprule\n",
    "\\textbf{Problem} & \"\"\" + \" & \".join([f\"{{{models_dict[t]}}}\" for t in types]) + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "rows = []\n",
    "for prob in problems:\n",
    "    row = [problems_dict[prob]]\n",
    "    for t in types:\n",
    "        entry = loss_summary[(loss_summary['Problem'] == prob) & (loss_summary['Type'] == t)]\n",
    "        if not entry.empty:\n",
    "            mean = entry['Mean_accuracy'].values[0]\n",
    "            sem = entry['Std_accuracy'].values[0]\n",
    "            formatted = format_mean_sem(mean, sem)\n",
    "            if best_types_per_problem[prob] == t:\n",
    "                formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "            else:\n",
    "                prob_Df = ttest_df[ttest_df.Problem == prob]\n",
    "                if prob_Df[prob_Df.Compared_Type == t].Best_Better.all() == False:\n",
    "                    formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "                else:\n",
    "                    formatted = r\"\\num{\" + formatted + \"}\"\n",
    "        else:\n",
    "            formatted = \"-\"\n",
    "        row.append(formatted)\n",
    "    rows.append(\" & \".join(row) + r\" \\\\\")\n",
    "\n",
    "footer = r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\"\"\"\n",
    "\n",
    "latex_table = header + \"\\n\".join(rows) + \"\\n\" + footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classification.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(p):\n",
    "    dfs = []\n",
    "    for folder in p.glob('*'):\n",
    "        for file in folder.glob('*.csv'):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "            dfs[-1]['Seed'] = int(re.findall(r'\\d+', file.name)[0])\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df['Loglr'] = df['Learning rate'].apply(lambda x: np.log10(x))\n",
    "\n",
    "    activation_dict = {\n",
    "        'activation.identity': \"\",\n",
    "        'activation.relu': \"ReLU\",\n",
    "        'activation.leaky_relu': \"LeakyReLU\",\n",
    "        'activation.sigmoid': \"Sigmoid\",\n",
    "        'activation.dmelu': \"DiLU\"\n",
    "    }\n",
    "\n",
    "    model_dict = {\n",
    "        \"model_type.hyperbolic\" : \"Cartan\",\n",
    "        \"model_type.euclidean\" : \"Euclidean\",\n",
    "        \"model_type.eubn\" : \"Eucl + BN\",\n",
    "        \"model_type.poincare\" : \"Poincare\",\n",
    "        \"model_type.lorentz\" : \"Lorentz\"\n",
    "    }\n",
    "\n",
    "    df['Model'] = df['Model'].apply(lambda x: model_dict[x])\n",
    "    df['Activation'] = df['Activation'].apply(lambda x: activation_dict[x])\n",
    "    df['Type'] = df['Model'] + ' - ' + df['Activation']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmnist = build_dataset(Path('../data/kmnist/kmnist'))\n",
    "cifar = build_dataset(Path('../data/cifar/cifar10'))\n",
    "\n",
    "dfs = []\n",
    "for df, prob in zip([kmnist, cifar], ['KMNIST', 'CIFAR10']):\n",
    "    df['Type'] = df['Model'] + '_' + df['Activation']\n",
    "    df['Type'] = df['Type'].apply(lambda x: ' '.join(x.split('_')))\n",
    "    df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons'], as_index=False).mean()\n",
    "    df1['Problem'] = prob\n",
    "    dfs.append(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_462662/1835849069.py:19: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/1835849069.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/1835849069.py:19: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_462662/1835849069.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n"
     ]
    }
   ],
   "source": [
    "configs = [[],[]]\n",
    "for df, dfc, prob in zip(dfs, configs, ['KMNIST', 'CIFAR10']):\n",
    "    df = df[df['Neurons']<500]\n",
    "    for type in df['Type'].unique():\n",
    "        best_conf = df[df['Type']==type].sort_values(by = 'Test accuracy', ascending=False).iloc[0]\n",
    "        neurons = best_conf['Neurons']\n",
    "        nlayers = best_conf['Nlayers']\n",
    "        dfc.append(\n",
    "            {\n",
    "                \"Type\":type,\n",
    "                \"Neurons\":neurons,\n",
    "                \"Nlayers\":nlayers,\n",
    "                \"Problem\":prob\n",
    "            }\n",
    "        )\n",
    "\n",
    "final_dfs = []\n",
    "for df, config_list in zip([kmnist, cifar], configs):\n",
    "    dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
    "    for config in config_list:\n",
    "        temp = dfagg[np.logical_and(np.logical_and(\n",
    "            dfagg['Type']==config['Type'],\n",
    "            dfagg['Neurons'] == config['Neurons']\n",
    "        ),\n",
    "        dfagg['Nlayers']==config['Nlayers']\n",
    "        )].copy()\n",
    "        temp['Problem'] = config['Problem']\n",
    "        final_dfs.append(temp)\n",
    "\n",
    "best_class_df = pd.concat(final_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary = (\n",
    "    best_class_df\n",
    "    .groupby(['Problem', 'Type'])['Test accuracy']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Mean_accuracy', 'std': 'Std_accuracy', 'count': 'N'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Type</th>\n",
       "      <th>Mean_accuracy</th>\n",
       "      <th>Std_accuracy</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Cartan</td>\n",
       "      <td>0.50292</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Cartan DiLU</td>\n",
       "      <td>0.53434</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Euclidean DiLU</td>\n",
       "      <td>0.53892</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Poincare DiLU</td>\n",
       "      <td>0.52645</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Cartan</td>\n",
       "      <td>0.84477</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Cartan DiLU</td>\n",
       "      <td>0.89178</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Euclidean DiLU</td>\n",
       "      <td>0.88630</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Poincare DiLU</td>\n",
       "      <td>0.87767</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Problem            Type  Mean_accuracy  Std_accuracy   N\n",
       "0  CIFAR10         Cartan         0.50292      0.006950   5\n",
       "1  CIFAR10     Cartan DiLU        0.53434      0.002985   5\n",
       "2  CIFAR10  Euclidean DiLU        0.53892      0.002422  10\n",
       "3  CIFAR10   Poincare DiLU        0.52645      0.002551  10\n",
       "4   KMNIST         Cartan         0.84477      0.005032  10\n",
       "5   KMNIST     Cartan DiLU        0.89178      0.002746  10\n",
       "6   KMNIST  Euclidean DiLU        0.88630      0.003007  10\n",
       "7   KMNIST   Poincare DiLU        0.87767      0.003514  10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_results = []\n",
    "\n",
    "for problem, group in best_class_df.groupby(\"Problem\"):\n",
    "\n",
    "    classification_means = group.groupby(\"Type\")[\"Test accuracy\"].mean()\n",
    "    best_type = classification_means.idxmax()\n",
    "    \n",
    "    best_scores = group[group[\"Type\"] == best_type][\"Test accuracy\"]\n",
    "    \n",
    "    for type_ in group[\"Type\"].unique():\n",
    "        if type_ == best_type:\n",
    "            continue\n",
    "        \n",
    "        comp_scores = group[group[\"Type\"] == type_][\"Test accuracy\"]\n",
    "        \n",
    "        t_stat, p_val = ttest_ind(best_scores, comp_scores, equal_var=False)  # Welch's t-test\n",
    "        \n",
    "        ttest_results.append({\n",
    "            \"Problem\": problem,\n",
    "            \"Best_Type\": best_type,\n",
    "            \"Compared_Type\": type_,\n",
    "            \"T_stat\": t_stat,\n",
    "            \"P_value\": p_val,\n",
    "            \"Best_Mean\": best_scores.mean(),\n",
    "            \"Compared_Mean\": comp_scores.mean(),\n",
    "            \"Best_Better\": best_scores.mean() > comp_scores.mean() and p_val < 0.05\n",
    "        })\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Best_Type</th>\n",
       "      <th>Compared_Type</th>\n",
       "      <th>T_stat</th>\n",
       "      <th>P_value</th>\n",
       "      <th>Best_Mean</th>\n",
       "      <th>Compared_Mean</th>\n",
       "      <th>Best_Better</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Euclidean DiLU</td>\n",
       "      <td>Cartan</td>\n",
       "      <td>11.246025</td>\n",
       "      <td>1.846873e-04</td>\n",
       "      <td>0.53892</td>\n",
       "      <td>0.50292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Euclidean DiLU</td>\n",
       "      <td>Cartan DiLU</td>\n",
       "      <td>2.975386</td>\n",
       "      <td>2.157442e-02</td>\n",
       "      <td>0.53892</td>\n",
       "      <td>0.53434</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CIFAR10</td>\n",
       "      <td>Euclidean DiLU</td>\n",
       "      <td>Poincare DiLU</td>\n",
       "      <td>11.209373</td>\n",
       "      <td>1.545149e-09</td>\n",
       "      <td>0.53892</td>\n",
       "      <td>0.52645</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Cartan DiLU</td>\n",
       "      <td>Cartan</td>\n",
       "      <td>25.931597</td>\n",
       "      <td>3.482275e-13</td>\n",
       "      <td>0.89178</td>\n",
       "      <td>0.84477</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Cartan DiLU</td>\n",
       "      <td>Euclidean DiLU</td>\n",
       "      <td>4.255641</td>\n",
       "      <td>4.834984e-04</td>\n",
       "      <td>0.89178</td>\n",
       "      <td>0.88630</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KMNIST</td>\n",
       "      <td>Cartan DiLU</td>\n",
       "      <td>Poincare DiLU</td>\n",
       "      <td>10.005220</td>\n",
       "      <td>1.531700e-08</td>\n",
       "      <td>0.89178</td>\n",
       "      <td>0.87767</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Problem       Best_Type   Compared_Type     T_stat       P_value  \\\n",
       "0  CIFAR10  Euclidean DiLU         Cartan   11.246025  1.846873e-04   \n",
       "1  CIFAR10  Euclidean DiLU     Cartan DiLU   2.975386  2.157442e-02   \n",
       "2  CIFAR10  Euclidean DiLU   Poincare DiLU  11.209373  1.545149e-09   \n",
       "3   KMNIST     Cartan DiLU         Cartan   25.931597  3.482275e-13   \n",
       "4   KMNIST     Cartan DiLU  Euclidean DiLU   4.255641  4.834984e-04   \n",
       "5   KMNIST     Cartan DiLU   Poincare DiLU  10.005220  1.531700e-08   \n",
       "\n",
       "   Best_Mean  Compared_Mean  Best_Better  \n",
       "0    0.53892        0.50292         True  \n",
       "1    0.53892        0.53434         True  \n",
       "2    0.53892        0.52645         True  \n",
       "3    0.89178        0.84477         True  \n",
       "4    0.89178        0.88630         True  \n",
       "5    0.89178        0.87767         True  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_dict = {\"CIFAR10\": \"Cifar10\", \n",
    "                 \"fmnist\":\"FMNIST\", \n",
    "                 \"KMNIST\":\"KMNIST\", \n",
    "                 \"mnist\":\"MNIST\"\n",
    "                 }\n",
    "\n",
    "models_dict = {\"Poincare DiLU\": \"Poincaré + DiLU\",\n",
    "               \"Cartan \": \"Cartan\",\n",
    "               \"Cartan DiLU\": \"Cartan + DiLU\",\n",
    "               \"Euclidean DiLU\": \"Euclidean + DiLU\",\n",
    "               \"Lorentz DELU\": \"Lorentz + DiLU\",\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = loss_summary['Problem'].unique()\n",
    "types = loss_summary['Type'].unique()\n",
    "\n",
    "best_types_per_problem = {}\n",
    "for prob in problems:\n",
    "    sig_bests = ttest_df[(ttest_df[\"Problem\"] == prob) & (ttest_df[\"Best_Better\"] == True)]\n",
    "    if not sig_bests.empty:\n",
    "        best_type = sig_bests.iloc[0][\"Best_Type\"]\n",
    "    else:\n",
    "        best_type = loss_summary[loss_summary[\"Problem\"] == prob].sort_values(\"R^2\", ascending=False).iloc[0][\"Type\"]\n",
    "    best_types_per_problem[prob] = best_type\n",
    "\n",
    "header = r\"\"\"\\centering\n",
    "\\caption{Best ccuracy on real-world datasets (mean $\\pm$ std, $n$ = \"\"\" + str(int(n)) + r\"\"\")}\\label{tab:bestclassification}\n",
    "\\begin{tabular}{\n",
    "  l\"\"\" + \"  \" + \"  \".join([\"S[table-format=1.3(2)]\" for _ in types]) + r\"\"\"}\n",
    "\\toprule\n",
    "\\textbf{Problem} & \"\"\" + \" & \".join([f\"{{{models_dict[t]}}}\" for t in types]) + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "rows = []\n",
    "for prob in problems:\n",
    "    row = [problems_dict[prob]]\n",
    "    for t in types:\n",
    "        entry = loss_summary[(loss_summary['Problem'] == prob) & (loss_summary['Type'] == t)]\n",
    "        if not entry.empty:\n",
    "            mean = entry['Mean_accuracy'].values[0]\n",
    "            sem = entry['Std_accuracy'].values[0]\n",
    "            formatted = format_mean_sem(mean, sem)\n",
    "            if best_types_per_problem[prob] == t:\n",
    "                formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "            else:\n",
    "                prob_Df = ttest_df[ttest_df.Problem == prob]\n",
    "                if prob_Df[prob_Df.Compared_Type == t].Best_Better.all() == False:\n",
    "                    formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "                else:\n",
    "                    formatted = r\"\\num{\" + formatted + \"}\"\n",
    "        else:\n",
    "            formatted = \"-\"\n",
    "        row.append(formatted)\n",
    "    rows.append(\" & \".join(row) + r\" \\\\\")\n",
    "\n",
    "footer = r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\"\"\"\n",
    "\n",
    "latex_table = header + \"\\n\".join(rows) + \"\\n\" + footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_classification.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best supplementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(p):\n",
    "    dfs = []\n",
    "    for folder in p.glob('*'):\n",
    "        for file in folder.glob('*.csv'):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "            dfs[-1]['Seed'] = int(re.findall(r'\\d+', file.name)[0])\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df['Loglr'] = df['Learning rate'].apply(lambda x: np.log10(x))\n",
    "\n",
    "    activation_dict = {\n",
    "        'activation.identity': \"\",\n",
    "        'activation.relu': \"ReLU\",\n",
    "        'activation.leaky_relu': \"LeakyReLU\",\n",
    "        'activation.sigmoid': \"Sigmoid\",\n",
    "        'activation.dmelu': \"DiLU\"\n",
    "    }\n",
    "\n",
    "    model_dict = {\n",
    "        \"model_type.hyperbolic\" : \"Cartan\",\n",
    "        \"model_type.euclidean\" : \"Euclidean\",\n",
    "        \"model_type.eubn\" : \"Eucl + BN\",\n",
    "        \"model_type.poincare\" : \"Poincare\",\n",
    "        \"model_type.lorentz\" : \"Lorentz\"\n",
    "    }\n",
    "\n",
    "    df['Model'] = df['Model'].apply(lambda x: model_dict[x])\n",
    "    df['Activation'] = df['Activation'].apply(lambda x: activation_dict[x])\n",
    "    df['Type'] = df['Model'] + ' - ' + df['Activation']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = build_dataset(Path('../data/mnist'))\n",
    "fmnist = build_dataset(Path('../data/fmnist'))\n",
    "\n",
    "dfs = []\n",
    "for df, prob in zip([mnist, fmnist], ['MNIST', 'fMNIST']):\n",
    "    df['Type'] = df['Model'] + '_' + df['Activation']\n",
    "    df['Type'] = df['Type'].apply(lambda x: ' '.join(x.split('_')))\n",
    "    df1 = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons'], as_index=False).mean()\n",
    "    df1['Problem'] = prob\n",
    "    dfs.append(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2951084/3924180933.py:19: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_2951084/3924180933.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_2951084/3924180933.py:19: FutureWarning: The provided callable <built-in function max> is currently using np.maximum.reduce. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.maximum.reduce instead.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
      "/tmp/ipykernel_2951084/3924180933.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n"
     ]
    }
   ],
   "source": [
    "configs = [[],[]]\n",
    "for df, dfc, prob in zip(dfs, configs, ['MNIST', 'fMNIST']):\n",
    "    df = df[df['Neurons']<500]\n",
    "    for type in df['Type'].unique():\n",
    "        best_conf = df[df['Type']==type].sort_values(by = 'Test accuracy', ascending=False).iloc[0]\n",
    "        neurons = best_conf['Neurons']\n",
    "        nlayers = best_conf['Nlayers']\n",
    "        dfc.append(\n",
    "            {\n",
    "                \"Type\":type,\n",
    "                \"Neurons\":neurons,\n",
    "                \"Nlayers\":nlayers,\n",
    "                \"Problem\":prob\n",
    "            }\n",
    "        )\n",
    "\n",
    "final_dfs = []\n",
    "for df, config_list in zip([mnist, fmnist], configs):\n",
    "    dfagg = df.groupby(['Model', 'Activation', 'Nlayers', 'Type', 'Neurons','Seed'], as_index=False).apply(max)\n",
    "    for config in config_list:\n",
    "        temp = dfagg[np.logical_and(np.logical_and(\n",
    "            dfagg['Type']==config['Type'],\n",
    "            dfagg['Neurons'] == config['Neurons']\n",
    "        ),\n",
    "        dfagg['Nlayers']==config['Nlayers']\n",
    "        )].copy()\n",
    "        temp['Problem'] = config['Problem']\n",
    "        final_dfs.append(temp)\n",
    "\n",
    "best_class_df = pd.concat(final_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary = (\n",
    "    best_class_df\n",
    "    .groupby(['Problem', 'Type'])['Test accuracy']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Mean_accuracy', 'std': 'Std_accuracy', 'count': 'N'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_results = []\n",
    "\n",
    "for problem, group in best_class_df.groupby(\"Problem\"):\n",
    "\n",
    "    classification_means = group.groupby(\"Type\")[\"Test accuracy\"].mean()\n",
    "    best_type = classification_means.idxmax()\n",
    "    \n",
    "    best_scores = group[group[\"Type\"] == best_type][\"Test accuracy\"]\n",
    "    \n",
    "    for type_ in group[\"Type\"].unique():\n",
    "        if type_ == best_type:\n",
    "            continue\n",
    "        \n",
    "        comp_scores = group[group[\"Type\"] == type_][\"Test accuracy\"]\n",
    "        \n",
    "        t_stat, p_val = ttest_ind(best_scores, comp_scores, equal_var=False)  # Welch's t-test\n",
    "        \n",
    "        ttest_results.append({\n",
    "            \"Problem\": problem,\n",
    "            \"Best_Type\": best_type,\n",
    "            \"Compared_Type\": type_,\n",
    "            \"T_stat\": t_stat,\n",
    "            \"P_value\": p_val,\n",
    "            \"Best_Mean\": best_scores.mean(),\n",
    "            \"Compared_Mean\": comp_scores.mean(),\n",
    "            \"Best_Better\": best_scores.mean() > comp_scores.mean() and p_val < 0.05\n",
    "        })\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_dict = {\"CIFAR10\": \"Cifar10\", \n",
    "                 \"fMNIST\":\"fMNIST\", \n",
    "                 \"KMNIST\":\"KMNIST\", \n",
    "                 \"MNIST\":\"MNIST\"\n",
    "                 }\n",
    "\n",
    "models_dict = {\"Poincare DiLU\": \"Poincaré + DiLU\",\n",
    "               \"Cartan \": \"Cartan\",\n",
    "               \"Cartan DiLU\": \"Cartan + DiLU\",\n",
    "               \"Euclidean DiLU\": \"Euclidean + DiLU\",\n",
    "               \"Lorentz DELU\": \"Lorentz + DiLU\",\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = loss_summary['Problem'].unique()\n",
    "types = loss_summary['Type'].unique()\n",
    "n = 10\n",
    "\n",
    "best_types_per_problem = {}\n",
    "for prob in problems:\n",
    "    sig_bests = ttest_df[(ttest_df[\"Problem\"] == prob) & (ttest_df[\"Best_Better\"] == True)]\n",
    "    if not sig_bests.empty:\n",
    "        best_type = sig_bests.iloc[0][\"Best_Type\"]\n",
    "    else:\n",
    "        best_type = loss_summary[loss_summary[\"Problem\"] == prob].sort_values(\"R^2\", ascending=False).iloc[0][\"Type\"]\n",
    "    best_types_per_problem[prob] = best_type\n",
    "\n",
    "header = r\"\"\"\\centering\n",
    "\\caption{Best accuracy on real-world datasets (mean $\\pm$ std, $n$ = \"\"\" + str(int(n)) + r\"\"\")}\\label{tab:suppbest}\n",
    "\\begin{tabular}{\n",
    "  l\"\"\" + \"  \" + \"  \".join([\"S[table-format=1.3(2)]\" for _ in types]) + r\"\"\"}\n",
    "\\toprule\n",
    "\\textbf{Problem} & \"\"\" + \" & \".join([f\"{{{models_dict[t]}}}\" for t in types]) + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "rows = []\n",
    "for prob in problems:\n",
    "    row = [problems_dict[prob]]\n",
    "    for t in types:\n",
    "        entry = loss_summary[(loss_summary['Problem'] == prob) & (loss_summary['Type'] == t)]\n",
    "        if not entry.empty:\n",
    "            mean = entry['Mean_accuracy'].values[0]\n",
    "            sem = entry['Std_accuracy'].values[0]\n",
    "            formatted = format_mean_sem(mean, sem)\n",
    "            if best_types_per_problem[prob] == t:\n",
    "                formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "            else:\n",
    "                prob_Df = ttest_df[ttest_df.Problem == prob]\n",
    "                if prob_Df[prob_Df.Compared_Type == t].Best_Better.all() == False:\n",
    "                    formatted = r\"\\cellcolor{yellow!30}{\\num{\" + formatted + \"}}\"\n",
    "                else:\n",
    "                    formatted = r\"\\num{\" + formatted + \"}\"\n",
    "        else:\n",
    "            formatted = \"-\"\n",
    "        row.append(formatted)\n",
    "    rows.append(\" & \".join(row) + r\" \\\\\")\n",
    "\n",
    "footer = r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\"\"\"\n",
    "\n",
    "latex_table = header + \"\\n\".join(rows) + \"\\n\" + footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"supp_classification.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbolic Alexnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(p):\n",
    "    dfs = []\n",
    "    for folder in p.glob('*'):\n",
    "        for file in folder.glob('*.csv'):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "            dfs[-1]['Seed'] = int(re.findall(r'\\d+', file.name)[0])\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df['Loglr'] = df['Learning rate'].apply(lambda x: np.log10(x))\n",
    "\n",
    "    model_dict = {\n",
    "        \"model_type.hyperbolic\" : \"Cartan\",\n",
    "        \"model_type.euclidean\" : \"Euclidean\",\n",
    "        \"model_type.eubn\" : \"Eucl + BN\",\n",
    "        \"model_type.poincare\" : \"Poincare\",\n",
    "        \"model_type.lorentz\" : \"Lorentz\",\n",
    "        \"model_type.halexnet\" : \"H-Alexnet\",\n",
    "        \"model_type.alexnet\" : \"Alexnet\",\n",
    "        \"model_type.resnet\" : \"ResNet18\",\n",
    "        \"model_type.hresnet\" : \"H-ResNet18\"\n",
    "    }\n",
    "\n",
    "    df['Model'] = df['Model'].apply(lambda x: model_dict[x])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebA = build_dataset(Path('../data/halexnet/celebA'))\n",
    "celebA['Problem'] = 'CelebA'\n",
    "cifar10 = build_dataset(Path('../data/halexnet/cifar10'))\n",
    "cifar10['Problem'] = 'Cifar10'\n",
    "cifar100 = build_dataset(Path('../data/halexnet/cifar100'))\n",
    "cifar100['Problem'] = 'Cifar100'\n",
    "tiny_imagenet = build_dataset(Path('../data/halexnet/tiny_imagenet'))\n",
    "tiny_imagenet['Problem'] = 'TinyImagenet'\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for df, prob in zip([celebA, cifar10, cifar100, tiny_imagenet], ['CelebA', 'Cifar10', 'Cifar100', 'TinyImagenet']):\n",
    "    df1 = df.groupby(['Model', 'Seed'], as_index=False).max()\n",
    "    df1['Problem'] = prob\n",
    "    dfs.append(df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [[],[],[],[]]\n",
    "for df, dfc, prob in zip(dfs, configs, ['CelebA', 'Cifar10', 'Cifar100', 'TinyImagenet']):\n",
    "    for type in df['Model'].unique():\n",
    "        best_conf = df[df['Model']==type].sort_values(by = 'Test accuracy', ascending=False).iloc[0]\n",
    "        dfc.append(\n",
    "            {\n",
    "                \"Model\":type,\n",
    "                \"Problem\":prob\n",
    "            }\n",
    "        )\n",
    "\n",
    "final_dfs = []\n",
    "for df, config_list in zip([celebA, cifar10, cifar100, tiny_imagenet], configs):\n",
    "    dfagg = df[['Model', 'Problem', 'Seed', 'Test accuracy']].groupby(['Model', 'Problem', 'Seed'], as_index=False,).apply(np.maximum.reduce, include_groups=False)\n",
    "    for config in config_list:\n",
    "        temp = dfagg[\n",
    "            dfagg['Model']==config['Model']\n",
    "        ].copy()\n",
    "        temp['Problem'] = config['Problem']\n",
    "        final_dfs.append(temp)\n",
    "\n",
    "best_class_df = pd.concat(final_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_results = []\n",
    "\n",
    "for problem, group in best_class_df.groupby(\"Problem\"):\n",
    "\n",
    "    classification_means = group.groupby(\"Model\")[\"Test accuracy\"].mean()\n",
    "    best_type = classification_means.idxmax()\n",
    "    \n",
    "    best_scores = group[group[\"Model\"] == best_type][\"Test accuracy\"]\n",
    "    \n",
    "    for type_ in group[\"Model\"].unique():\n",
    "        if type_ == best_type:\n",
    "            continue\n",
    "        \n",
    "        comp_scores = group[group[\"Model\"] == type_][\"Test accuracy\"]\n",
    "        \n",
    "        t_stat, p_val = ttest_ind(best_scores, comp_scores, equal_var=False)  # Welch's t-test\n",
    "        \n",
    "        ttest_results.append({\n",
    "            \"Problem\": problem,\n",
    "            \"Best_Type\": best_type,\n",
    "            \"Compared_Type\": type_,\n",
    "            \"T_stat\": t_stat,\n",
    "            \"P_value\": p_val,\n",
    "            \"Best_Mean\": best_scores.mean(),\n",
    "            \"Compared_Mean\": comp_scores.mean(),\n",
    "            \"Best_Better\": best_scores.mean() > comp_scores.mean() and p_val < 0.05\n",
    "        })\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_summary = (\n",
    "    best_class_df\n",
    "    .groupby(['Problem', 'Model'])['Test accuracy']\n",
    "    .agg(['mean', 'std', 'count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Mean_accuracy', 'std': 'Std_accuracy', 'count': 'N'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean_accuracy</th>\n",
       "      <th>Std_accuracy</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>Alexnet</td>\n",
       "      <td>0.778392</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>H-Alexnet</td>\n",
       "      <td>0.774023</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cifar10</td>\n",
       "      <td>Alexnet</td>\n",
       "      <td>0.884158</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cifar10</td>\n",
       "      <td>H-Alexnet</td>\n",
       "      <td>0.883529</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cifar100</td>\n",
       "      <td>Alexnet</td>\n",
       "      <td>0.543598</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cifar100</td>\n",
       "      <td>H-Alexnet</td>\n",
       "      <td>0.595095</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TinyImagenet</td>\n",
       "      <td>Alexnet</td>\n",
       "      <td>0.380518</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TinyImagenet</td>\n",
       "      <td>H-Alexnet</td>\n",
       "      <td>0.446338</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Problem      Model  Mean_accuracy  Std_accuracy  N\n",
       "0        CelebA    Alexnet       0.778392      0.004887  5\n",
       "1        CelebA  H-Alexnet       0.774023      0.006775  5\n",
       "2       Cifar10    Alexnet       0.884158      0.000929  5\n",
       "3       Cifar10  H-Alexnet       0.883529      0.004624  5\n",
       "4      Cifar100    Alexnet       0.543598      0.002891  5\n",
       "5      Cifar100  H-Alexnet       0.595095      0.008262  5\n",
       "6  TinyImagenet    Alexnet       0.380518      0.007024  5\n",
       "7  TinyImagenet  H-Alexnet       0.446338      0.003162  5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_dict = {\"CelebA\": \"CelebA\", \n",
    "                 \"Cifar10\":\"Cifar10\", \n",
    "                 \"Cifar100\":\"Cifar100\", \n",
    "                 \"TinyImagenet\":\"TinyImagenet\"\n",
    "                 }\n",
    "\n",
    "models_dict = {\"Alexnet\": \"Alexnet\",\n",
    "               \"H-Alexnet\": \"H-Alexnet\",\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def format_mean_sem(mean, sem):\n",
    "    if sem == 0 or np.isnan(sem):\n",
    "        return f\"{mean:.4f}(0)\"  # fallback\n",
    "    \n",
    "    exponent = int(math.floor(np.log10(sem)))\n",
    "    precision = -exponent\n",
    "    digits = max(0, precision)\n",
    "\n",
    "    sem_rounded = round(sem, digits)\n",
    "    mean_rounded = round(mean, digits)\n",
    "    sem_for_latex = int(round(sem_rounded * (10 ** digits)))\n",
    "\n",
    "    return f\"{mean_rounded:.{digits}f}({sem_for_latex})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = loss_summary['Problem'].unique()\n",
    "types = loss_summary['Model'].unique()\n",
    "n = 5\n",
    "\n",
    "best_types_per_problem = {}\n",
    "for prob in problems:\n",
    "    sig_bests = ttest_df[(ttest_df[\"Problem\"] == prob) & (ttest_df[\"Best_Better\"] == True)]\n",
    "    if not sig_bests.empty:\n",
    "        best_type = sig_bests.iloc[0][\"Best_Type\"]\n",
    "    else:\n",
    "        best_type = loss_summary[loss_summary[\"Problem\"] == prob].sort_values(\"Mean_accuracy\", ascending=False).iloc[0][\"Model\"]\n",
    "    best_types_per_problem[prob] = best_type\n",
    "\n",
    "header = r\"\"\"\\centering\n",
    "\\caption{Best accuracy on real-world datasets (mean $\\pm$ std, $n$ = \"\"\" + str(int(n)) + r\"\"\")}\\label{tab:suppbest}\n",
    "\\begin{tabular}{\n",
    "  l\"\"\" + \"  \" + \"  \".join([\"S[table-format=1.3(2)]\" for _ in types]) + r\"\"\"}\n",
    "\\toprule\n",
    "\\textbf{Problem} & \"\"\" + \" & \".join([f\"{{{models_dict[t]}}}\" for t in types]) + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "rows = []\n",
    "for prob in problems:\n",
    "    row = [problems_dict[prob]]\n",
    "    for t in types:\n",
    "        entry = loss_summary[(loss_summary['Problem'] == prob) & (loss_summary['Model'] == t)]\n",
    "        if not entry.empty:\n",
    "            mean = entry['Mean_accuracy'].values[0]\n",
    "            sem = entry['Std_accuracy'].values[0]\n",
    "            formatted = format_mean_sem(mean, sem)\n",
    "            if best_types_per_problem[prob] == t:\n",
    "                formatted = r\"\\underline{\\num{\" + formatted + \"}}\"\n",
    "            else:\n",
    "                prob_Df = ttest_df[ttest_df.Problem == prob]\n",
    "                if prob_Df[prob_Df.Compared_Type == t].Best_Better.all() == False:\n",
    "                    formatted = r\"\\underline{\\num{\" + formatted + \"}}\"\n",
    "                else:\n",
    "                    formatted = r\"\\num{\" + formatted + \"}\"\n",
    "        else:\n",
    "            formatted = \"-\"\n",
    "        row.append(formatted)\n",
    "    rows.append(\" & \".join(row) + r\" \\\\\")\n",
    "\n",
    "footer = r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\"\"\"\n",
    "\n",
    "latex_table = header + \"\\n\".join(rows) + \"\\n\" + footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alexnet_classification.txt\", \"w\") as text_file:\n",
    "    text_file.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best resnet - parameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def build_dataset(p: Path):\n",
    "    dfs = []\n",
    "    for folder in p.glob('*/*'):\n",
    "        for file in folder.glob('*.csv'):\n",
    "            dfs.append(pd.read_csv(file))\n",
    "            dfs[-1]['Seed'] = int(re.findall(r'\\d+', file.name)[0])\n",
    "            dfs[-1]['Dataset'] = folder.parent.name\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df['Loglr'] = df['Learning rate'].apply(lambda x: np.log10(x))\n",
    "\n",
    "    model_dict = {\n",
    "        \"model_type.resnet18\" : \"ResNet18\",\n",
    "        \"model_type.hresnet18\" : \"H-ResNet18\"\n",
    "    }\n",
    "\n",
    "    df['Model'] = df['Model'].apply(lambda x: model_dict[x])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1530990/210102529.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gdf = df[['Model', 'Dataset', 'Loglr', 'Weight decay', 'Test accuracy', 'Seed']].groupby(['Model', 'Dataset', 'Loglr', 'Weight decay', 'Seed'], as_index=False).apply(np.maximum.reduce)\n"
     ]
    }
   ],
   "source": [
    "df = build_dataset(Path('../data/all_conv_aug'))\n",
    "gdf = df[['Model', 'Dataset', 'Loglr', 'Weight decay', 'Test accuracy', 'Seed']].groupby(['Model', 'Dataset', 'Loglr', 'Weight decay', 'Seed'], as_index=False).apply(np.maximum.reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1530990/3065925979.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gdf1 = gdf.groupby(['Model', 'Dataset', 'Loglr', 'Weight decay']).apply(lambda x: list(x['Test accuracy'])).reset_index()\n"
     ]
    }
   ],
   "source": [
    "gdf1 = gdf.groupby(['Model', 'Dataset', 'Loglr', 'Weight decay']).apply(lambda x: list(x['Test accuracy'])).reset_index()\n",
    "gdf1['list'] = gdf1[0]\n",
    "gdf1['mean'] = gdf1['list'].apply(np.mean)\n",
    "gdf1['std'] = gdf1['list'].apply(np.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1530990/1250804235.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gdf1best = gdf1.groupby(['Model', 'Dataset', 'Weight decay']).apply(group_to_best).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def group_to_best(x):\n",
    "    index = np.argmax(x['mean'])\n",
    "    best_lr = (x['Loglr'].iloc[index])\n",
    "    best_list = x['list'].iloc[index]\n",
    "    return pd.Series([best_lr, best_list], index = ['Loglr', 'Results'])\n",
    "\n",
    "\n",
    "gdf1best = gdf1.groupby(['Model', 'Dataset', 'Weight decay']).apply(group_to_best).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Weight decay</th>\n",
       "      <th>Loglr</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-ResNet18</td>\n",
       "      <td>tiny_imagenet_224</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[0.6157852564102564, 0.6137820512820513, 0.617...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ResNet18</td>\n",
       "      <td>tiny_imagenet_224</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[0.6127804487179487, 0.6121794871794872, 0.618...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model            Dataset  Weight decay  Loglr  \\\n",
       "0  H-ResNet18  tiny_imagenet_224        0.0005   -1.0   \n",
       "1    ResNet18  tiny_imagenet_224        0.0005   -1.0   \n",
       "\n",
       "                                             Results  \n",
       "0  [0.6157852564102564, 0.6137820512820513, 0.617...  \n",
       "1  [0.6127804487179487, 0.6121794871794872, 0.618...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf1best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1530990/2609585162.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dffinal = gdf1best.groupby(['Dataset']).apply(group_comparison_winner).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def group_comparison_winner(x:pd.DataFrame):\n",
    "    stats = np.zeros((len(x), len(x)))\n",
    "    ps = np.zeros_like(stats)\n",
    "    means = np.zeros_like(stats)\n",
    "    stds = np.zeros_like(stats)\n",
    "    x = x.sort_values(by='Model', ascending=False)\n",
    "    for i, results1 in enumerate(x['Results']):\n",
    "        for j, results2 in enumerate(x['Results']):\n",
    "            stats[i,j], ps[i,j] = ttest_ind(results1, results2)\n",
    "            means[i,j] = np.mean(results1)\n",
    "            stds[i,j] = np.std(results1)\n",
    "    plausible_winner = np.argmax(means[:,0], axis=0)\n",
    "    stats = stats[plausible_winner]\n",
    "    ps = ps[plausible_winner]\n",
    "    return pd.Series(data = [*zip(means[:,0], stds[:,0]), plausible_winner, stats, ps],\n",
    "        index = x['Model'].to_list()  + ['Winner', 'Stat', 'p-value'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dffinal = gdf1best.groupby(['Dataset']).apply(group_comparison_winner).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>ResNet18</th>\n",
       "      <th>H-ResNet18</th>\n",
       "      <th>Winner</th>\n",
       "      <th>Stat</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiny_imagenet_224</td>\n",
       "      <td>(0.6136217948717949, 0.002421807894183897)</td>\n",
       "      <td>(0.6153445512820512, 0.001150056097308262)</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2851576526052522, 0.0]</td>\n",
       "      <td>[0.23469286330737246, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dataset                                    ResNet18  \\\n",
       "0  tiny_imagenet_224  (0.6136217948717949, 0.002421807894183897)   \n",
       "\n",
       "                                   H-ResNet18  Winner  \\\n",
       "0  (0.6153445512820512, 0.001150056097308262)       1   \n",
       "\n",
       "                        Stat                     p-value  \n",
       "0  [1.2851576526052522, 0.0]  [0.23469286330737246, 1.0]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    'celebA': 'CelebA',\n",
    "    'cifar10': 'Cifar10',\n",
    "    'cifar100': 'Cifar100',\n",
    "    'tiny_imagenet' : 'TinyImagenet(64)',\n",
    "    'tiny_imagenet_224' : 'TinyImagenet(224)'\n",
    "}\n",
    "dffinal['Dataset'] = dffinal['Dataset'].apply(lambda x: dataset_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_row_to_latex(row, nmodels=2):\n",
    "    results = row[1:1+nmodels]\n",
    "    accuracy_list = [f\"\\\\num{{{100*r[0]:.1f}({int(r[1]*10**3):1d})}}\" for r in results]\n",
    "    accuracy_list = [\"\\\\underline{\" + x + \"}\" if p>5e-2 else x for x,p in zip(accuracy_list,row['p-value'])]\n",
    "    return \" & \".join([row['Dataset']] + accuracy_list) + \" \\\\\\\\\\n\"\n",
    "\n",
    "def header(df:pd.DataFrame, nmodels=2):\n",
    "    return  \" & \".join([\"\\\\textbf{{Problem}}\"]+ [f\"{{{x}}}\" for x in df.columns[1:1+nmodels].tolist()]) + \" \\\\\\\\\\n\\\\midrule\\n\"\n",
    "\n",
    "def formatter(df, nmodels=2):\n",
    "    return \"\\\\begin{tabular}{ l \" + (\" S[table-format=1.3(2)] \" * nmodels) + \"}\\n\\\\toprule\\n\"\n",
    "\n",
    "def table(df:pd.DataFrame, nmodels=2):\n",
    "    preamble = (\"\\\\begin{table}\\n\\\\centering\\n\\\\caption{Resnet accuracy on real world datasets \" + \n",
    "    \"(mean $\\\\pm$ std, $n$ = 5)}\\\\label{tab:resnet}\\n\")\n",
    "    formt = formatter(df, nmodels)\n",
    "    rows = ''.join([header(df, nmodels)] + [convert_df_row_to_latex(x[1],nmodels) for x in df.iterrows()])\n",
    "    exit = \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\"\n",
    "    return preamble + formt + rows + exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resnet_classification.txt\", \"w\") as text_file:\n",
    "    text_file.write(table(dffinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hyperbolic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
